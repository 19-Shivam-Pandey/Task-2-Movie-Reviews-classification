Let's break down the output of the final cell of the Jupyter notebook, which includes the accuracy and the classification report.

### Output Explanation

1. **Accuracy**:
   ```plaintext
   Accuracy: 87.50%
   ```
   - **Accuracy** is a measure of the overall correctness of the model. It represents the percentage of correctly classified instances out of the total instances. For example, if the model has an accuracy of 87.50%, it means that 87.50% of the reviews were correctly classified as either positive or negative.

2. **Classification Report**:
   ```plaintext
   Classification Report:
                 precision    recall  f1-score   support

       Negative       0.85      0.89      0.87      2490
       Positive       0.90      0.86      0.88      2510

       accuracy                           0.88      5000
      macro avg       0.88      0.88      0.88      5000
   weighted avg       0.88      0.88      0.88      5000
   ```

   The classification report provides detailed metrics for each class (Negative and Positive).

   - **Precision**:
     - **Negative**: 0.85
     - **Positive**: 0.90
     - **Precision** is the ratio of correctly predicted positive observations to the total predicted positives. It tells us what percentage of the reviews predicted as positive are actually positive. Higher precision means fewer false positives.

   - **Recall**:
     - **Negative**: 0.89
     - **Positive**: 0.86
     - **Recall** is the ratio of correctly predicted positive observations to all observations in the actual class. It tells us what percentage of the actual positive reviews were correctly identified by the model. Higher recall means fewer false negatives.

   - **F1-Score**:
     - **Negative**: 0.87
     - **Positive**: 0.88
     - The **F1-Score** is the weighted average of precision and recall. It considers both false positives and false negatives. It is particularly useful when the class distribution is imbalanced.

   - **Support**:
     - **Negative**: 2490
     - **Positive**: 2510
     - **Support** is the number of actual occurrences of each class in the test set.

   - **Overall Accuracy**: 0.88
     - The overall accuracy of 0.88 (or 88%) indicates the proportion of total correctly classified instances.

   - **Macro Average**:
     - **Precision**: 0.88
     - **Recall**: 0.88
     - **F1-Score**: 0.88
     - **Macro Average** is the unweighted mean of the precision, recall, and F1-Score for each class. It treats all classes equally.

   - **Weighted Average**:
     - **Precision**: 0.88
     - **Recall**: 0.88
     - **F1-Score**: 0.88
     - **Weighted Average** is the average of the precision, recall, and F1-Score, weighted by the number of instances in each class. It takes into account the support of each class.

### Summary
- The model has an overall accuracy of 87.50%, meaning it correctly classified 87.50% of the reviews.
- The classification report breaks down the performance of the model for each class (Negative and Positive).
  - Precision, recall, and F1-score are high for both classes, indicating good performance.
  - The support shows the number of instances for each class, providing context for the precision, recall, and F1-score metrics.
- The macro average treats each class equally, while the weighted average considers the number of instances in each class.

These metrics give a comprehensive overview of the model's performance and highlight its strengths and weaknesses in classifying movie reviews as positive or negative.


LETS RUN THE CODE ONE AGAIN TO SHOW IF ITS IS WORKING OK P-----